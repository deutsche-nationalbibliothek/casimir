% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_ranked_retrieval_scores.r
\name{compute_ranked_retrieval_scores}
\alias{compute_ranked_retrieval_scores}
\title{Compute ranked retrieval scores}
\usage{
compute_ranked_retrieval_scores(
  gold_standard,
  predicted,
  doc_strata = NULL,
  compute_bootstrap_ci = FALSE,
  n_bt = 10L,
  seed = NULL,
  .progress = FALSE
)
}
\arguments{
\item{gold_standard}{gold-standard data, expects \code{data.frame} with cols
\emph{"label_id", "doc_id"}}

\item{predicted}{multi-label prediction results. expects \code{data.frame}
with cols \emph{"label_id", "doc_id", "score"}}

\item{doc_strata}{a column that exists in gold_standard,
that results should be grouped by, e.g. strata of document-space.
\code{doc_strata} is recommended to be of type factor, so that levels are
not implicitly dropped during bootstrap replications}

\item{compute_bootstrap_ci}{logical indicator for computing bootstrap CIs}

\item{n_bt}{an integer number of resamples to undergo in bootstrapping}

\item{seed}{pass seed to make bootstrap replication reproducible}

\item{.progress}{logical activating .progress bar in internal
\pkg{furrr}-computation}
}
\value{
a \code{data.frame} with cols
\emph{"metric", "mode", "value", "support"}
and optionally grouping
variables supplied in doc_strata. Here, \strong{support}
is defined number of documents that contribute to the document average
in aggregation of the overall result.
}
\description{
Rankied retrieval, unlike set retrieval, assumes ordered predictions. This
function computes dcg, ndcg, idcg and lrap
}
\details{
Unlike set retrieval metrics, ranked retrieval metrics are logically bound to
a document wise evaluation. Thus, there is only the aggregation mode
"doc-avg" for these scores available.
}
\examples{
# some dummy results
gold <- tibble::tribble(
  ~doc_id, ~label_id,
  "A", "a",
  "A", "b",
  "A", "c",
  "A", "d",
  "A", "e",
)

pred <- tibble::tribble(
  ~doc_id, ~label_id, ~score,
  "A", "f",	0.3277,
  "A", "e",	0.32172,
  "A", "b",	0.13517,
  "A", "g",	0.10134,
  "A", "h",	0.09152,
  "A", "a",	0.07483,
  "A", "i",	0.03649,
  "A", "j",	0.03551,
  "A", "k",	0.03397,
  "A", "c",	0.03364
)

results <- compute_ranked_retrieval_scores(
    gold,
    pred,
    compute_bootstrap_ci = FALSE
)

}
