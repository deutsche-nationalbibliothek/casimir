% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_set_retrieval_scores.r
\name{compute_set_retrieval_scores}
\alias{compute_set_retrieval_scores}
\alias{compute_set_retrieval_scores_dplyr}
\title{Compute multi label metrics for gnd subject indexing results}
\usage{
compute_set_retrieval_scores(
  gold_standard,
  predicted,
  mode = "doc-avg",
  compute_bootstrap_ci = FALSE,
  n_bt = 10L,
  doc_groups = NULL,
  label_groups = NULL,
  graded_relevance = FALSE,
  rename_graded_metrics = FALSE,
  seed = NULL,
  propensity_scored = FALSE,
  label_distribution = NULL,
  cost_fp_constant = NULL,
  ignore_inconsistencies = options::opt("ignore_inconsistencies"),
  verbose = options::opt("verbose"),
  progress = options::opt("progress")
)

compute_set_retrieval_scores_dplyr(
  gold_standard,
  predicted,
  mode = "doc-avg",
  compute_bootstrap_ci = FALSE,
  n_bt = 10L,
  doc_groups = NULL,
  label_groups = NULL,
  graded_relevance = FALSE,
  rename_graded_metrics = FALSE,
  seed = NULL,
  propensity_scored = FALSE,
  label_distribution = NULL,
  cost_fp_constant = NULL,
  ignore_inconsistencies = FALSE,
  verbose = FALSE,
  progress = FALSE
)
}
\arguments{
\item{gold_standard}{expects \code{data.frame} with cols
\emph{"label_id", "doc_id"}}

\item{predicted}{multi-label prediction results. expects \code{data.frame}
with cols \emph{"label_id", "doc_id"}}

\item{mode}{aggregation mode: \emph{"doc-avg", "subj-avg", "micro"}}

\item{compute_bootstrap_ci}{logical indicator for computing bootstrap CIs}

\item{n_bt}{an integer number of resamples to undergo in bootstrapping}

\item{doc_groups}{two-column \code{data.frame} with col \emph{"doc_id"}
and a second column that defines groups of labels to stratify results by.
It is recommended that groups are of type factor, so that levels are
not implicitly dropped during bootstrap replications}

\item{label_groups}{two-column \code{data.frame} with col \emph{"label_id"}
and a second column that defines groups of labels to stratify results by.
Results in each stratum
will restrict gold_standard and predictions to the specified label-groups,
as if the vocabulary was consisting of the label group only.
All modes \code{c("doc-avg", "subj-avg", "micro") } are supported within
label-strata.
Nevertheless, mixing \code{mode = "doc-avg"} with fine-grained
label_strata can result in many missing values on document-level results.
Also rank-based thresholding (e.g. Top-5) will result in inhomogeneous
number of labels per documents within the defined label-strata.
\code{mode = "subj-avg"} or \code{mode = "micro"} can be more appropriate
in these circumstances.}

\item{graded_relevance}{logical indicator for graded relevance. Defaults to
\code{FALSE} for binary relevance. If set to \code{TRUE}, the
\code{predicted} data.frame should contain a numeric column
\emph{"relevance"} with values in the range of \code{c(0, 1)}.}

\item{rename_graded_metrics}{if set to \code{TRUE}, the metric names in
the output will be prefixed with \emph{"g-"} to indicate that metrics
are computed with graded relevance.}

\item{seed}{pass seed to make bootstrap replication reproducible}

\item{propensity_scored}{logical, whether to use propensity scores as weights}

\item{label_distribution}{expects \code{data.frame} with cols
\emph{"label_id", "label_freq", "n_docs"}. \code{label_freq} corresonds to
the number of occurences a label has in the gold_standard. \code{n_docs}
corresponds to the total number of documents in the gold_standard.}

\item{cost_fp_constant}{Constant cost assigned to false positives.
cost_fp_constant must be
a numeric value > 0 or one of 'max', 'min', 'mean' (computed with reference
to the \code{gold_standard} label distribution). The default is NULL, i.e.
label weights are applied to false positives as to false negatives and
true positives.}

\item{ignore_inconsistencies}{Warnings about data inconsistencies will be silenced. (Defaults to \code{FALSE}, overwritable using option 'casimir.ignore_inconsistencies' or environment variable 'R_CASIMIR_IGNORE_INCONSISTENCIES')}

\item{verbose}{Verbose reorting of computation steps for debugging (Defaults to \code{FALSE}, overwritable using option 'casimir.verbose' or environment variable 'R_CASIMIR_VERBOSE')}

\item{progress}{Display progress bars for iterated computations (like bootstrap CI or
PR-Curves) (Defaults to \code{FALSE}, overwritable using option 'casimir.progress' or environment variable 'R_CASIMIR_PROGRESS')}
}
\value{
a \code{data.frame} with cols
\emph{"metric", "mode", "value", "support"}
and optionally grouping
variables supplied in doc_groups or label_groups. Here, \strong{support}
is defined for each \emph{mode} as,
\describe{
\item{\code{mode = "doc-avg"}}{the number of tested documents}
\item{\code{mode = "subj-avg"}}{the number of labels contributing to the subj-average} # nolint
\item{\code{mode = "micro"}}{the number of doc-label-pairs contributing
to the denominator of the respective metric, e.g. tp + fp for precision,
tp + fn for recall, tp + (fp + fn)/2 for f1 and min(tp + fp, tp + fn)
for r-precision.}
}
}
\description{
Compute multi label metrics for gnd subject indexing results
}
\section{Functions}{
\itemize{
\item \code{compute_set_retrieval_scores_dplyr()}: variant with internal usage of
dplyr rather than collapse library. Tends to be slower, but more stable

}}
\examples{

library(tidyverse)
library(casimir)
library(furrr)

gold <- tibble::tribble(
  ~doc_id, ~label_id,
  "A", "a",
  "A", "b",
  "A", "c",
  "B", "a",
  "B", "d",
  "C", "a",
  "C", "b",
  "C", "d",
  "C", "f",
)

pred <- tibble::tribble(
  ~doc_id, ~label_id,
  "A", "a",
  "A", "d",
  "A", "f",
  "B", "a",
  "B", "e",
  "C", "f",
)

plan(sequential) # or whatever resources you have

a <- compute_set_retrieval_scores(
gold, pred,
mode = "doc-avg",
compute_bootstrap_ci = TRUE,
n_bt = 100L)


ggplot(a, aes(x = metric, y = value)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(vars(metric), scales = "free")

# Example with graded relevance
pred_w_relevance <- tibble::tribble(
  ~doc_id, ~label_id, ~relevance,
  "A", "a", 1.0,
  "A", "d", 0.0,
  "A", "f", 0.0,
  "B", "a", 1.0,
  "B", "e", 1/3,
  "C", "f", 1.0,
)

b <- compute_set_retrieval_scores(
  gold, pred_w_relevance,
  mode = "doc-avg",
  graded_relevance = TRUE
)


}
