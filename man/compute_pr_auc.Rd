% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_pr_auc.r
\name{compute_pr_auc}
\alias{compute_pr_auc}
\title{Compute Area under precision recall curve with support for bootstrap based
confidence intervals and different stratification and aggregation modes
for the underlying precision and recall aggregation
Precision is calculated as the best value at a given level of recall for
all possible thresholds on score and limits on rank. In essence,
compute_pr_auc performs a two dimensional optimisation over thresholds and
limits applying both threshold-based cutoff as well as rank-based cutoff.}
\usage{
compute_pr_auc(
  gold_standard,
  predicted,
  doc_groups = NULL,
  label_groups = NULL,
  mode = "doc-avg",
  steps = 100,
  thresholds = NULL,
  limit_range = NA_real_,
  compute_bootstrap_ci = FALSE,
  n_bt = 10L,
  seed = NULL,
  graded_relevance = FALSE,
  rename_graded_metrics = FALSE,
  propensity_scored = FALSE,
  label_distribution = NULL,
  cost_fp_constant = NULL,
  ignore_inconsistencies = options::opt("ignore_inconsistencies"),
  verbose = options::opt("verbose"),
  progress = options::opt("progress")
)
}
\arguments{
\item{gold_standard}{expects \code{data.frame} with cols
\emph{"label_id", "doc_id"}}

\item{predicted}{multi-label prediction results. expects \code{data.frame}
with cols \emph{"label_id", "doc_id", "score"}}

\item{doc_groups}{two-column \code{data.frame} with col \emph{"doc_id"}
and a second column that defines groups of labels to stratify results by.
It is recommended that groups are of type factor, so that levels are
not implicitly dropped during bootstrap replications}

\item{label_groups}{two-column \code{data.frame} with col \emph{"label_id"}
and a second column that defines groups of labels to stratify results by.
Results in each stratum
will restrict gold_standard and predictions to the specified label-groups,
as if the vocabulary was consisting of the label group only.
All modes \code{c("doc-avg", "subj-avg", "micro") } are supported within
label-strata.
Nevertheless, mixing \code{mode = "doc-avg"} with fine-grained
label_strata can result in many missing values on document-level results.
Also rank-based thresholding (e.g. Top-5) will result in inhomogeneous
number of labels per documents within the defined label-strata.
\code{mode = "subj-avg"} or \code{mode = "micro"} can be more appropriate
in these circumstances.}

\item{mode}{aggregation mode: \emph{"doc-avg", "subj-avg", "micro"}}

\item{steps}{how many steps to take between c(0,1) as a grid for computing
the pr-curve}

\item{thresholds}{alternatively to steps, one can manually set, which
thresholds are used to build the pr_curve. Defaults to \code{steps}-percentiles
of the score distribution of true positives suggestions}

\item{limit_range}{a vector of limit values to apply on rank-column.
Defaults to NA, applying no cutoff on label-rank of predictions.}

\item{compute_bootstrap_ci}{logical indicator for computing bootstrap CIs}

\item{n_bt}{an integer number of resamples to undergo in bootstrapping}

\item{seed}{pass seed to make bootstrap replication reproducible}

\item{graded_relevance}{logical indicator for graded relevance. Defaults to
\code{FALSE} for binary relevance. If set to \code{TRUE}, the
\code{predicted} data.frame should contain a numeric column
\emph{"relevance"} with values in the range of \code{c(0, 1)}.}

\item{rename_graded_metrics}{if set to \code{TRUE}, the metric names in
the output will be prefixed with \emph{"g-"} to indicate that metrics
are computed with graded relevance.}

\item{propensity_scored}{logical, whether to use propensity scores as weights}

\item{label_distribution}{expects \code{data.frame} with cols
\emph{"label_id", "label_freq", "n_docs"}. \code{label_freq} corresonds to
the number of occurences a label has in the gold_standard. \code{n_docs}
corresponds to the total number of documents in the gold_standard.}

\item{cost_fp_constant}{Constant cost assigned to false positives.
cost_fp_constant must be
a numeric value > 0 or one of 'max', 'min', 'mean' (computed with reference
to the \code{gold_standard} label distribution). The default is NULL, i.e.
label weights are applied to false positives as to false negatives and
true positives.}

\item{ignore_inconsistencies}{Warnings about data inconsistencies will be silenced. (Defaults to \code{FALSE}, overwritable using option 'casimir.ignore_inconsistencies' or environment variable 'R_CASIMIR_IGNORE_INCONSISTENCIES')}

\item{verbose}{Verbose reorting of computation steps for debugging (Defaults to \code{FALSE}, overwritable using option 'casimir.verbose' or environment variable 'R_CASIMIR_VERBOSE')}

\item{progress}{Display progress bars for iterated computations (like bootstrap CI or
PR-Curves) (Defaults to \code{FALSE}, overwritable using option 'casimir.progress' or environment variable 'R_CASIMIR_PROGRESS')}
}
\value{
a \code{data.frame} with cols pr_auc and (if applicable)
ci_lower, ci_upper and additional stratification variables
}
\description{
Compute Area under precision recall curve with support for bootstrap based
confidence intervals and different stratification and aggregation modes
for the underlying precision and recall aggregation
Precision is calculated as the best value at a given level of recall for
all possible thresholds on score and limits on rank. In essence,
compute_pr_auc performs a two dimensional optimisation over thresholds and
limits applying both threshold-based cutoff as well as rank-based cutoff.
}
\examples{
#' library(ggplot2)
library(casimir)

gold <- tibble::tribble(
  ~doc_id, ~label_id,
  "A", "a",
  "A", "b",
  "A", "c",
  "B", "a",
  "B", "d",
  "C", "a",
  "C", "b",
  "C", "d",
  "C", "f"
)

pred <- tibble::tribble(
  ~doc_id, ~label_id, ~score, ~rank,
  "A", "a", 0.9, 1,
  "A", "d", 0.7, 2,
  "A", "f", 0.3, 3,
  "A", "c", 0.1, 4,
  "B", "a", 0.8, 1,
  "B", "e", 0.6, 2,
  "B", "d", 0.1, 3,
  "C", "f", 0.1, 3,
  "C", "c", 0.2, 1,
  "C", "e", 0.2, 1
)

auc <- compute_pr_auc(gold, pred, mode = "doc-avg")
}
\seealso{
compute_set_retrieval_scores compute_pr_auc_from_curve
}
