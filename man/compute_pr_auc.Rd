% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_pr_auc.r
\name{compute_pr_auc}
\alias{compute_pr_auc}
\title{Compute Area under precision recall curve with support for bootstrap based
confidence intervals and different stratification and aggregation modes
for the underlying precision and recall aggregation
Precision is calculated as the best value at a given level of recall for
all possible thresholds on score and limits on rank. In essence,
compute_pr_auc performs a two dimensional optimisation over thresholds and
limits applying both threshold-based cutoff as well as rank-based cutoff.}
\usage{
compute_pr_auc(
  gold_standard,
  predicted,
  doc_strata = NULL,
  label_dict = NULL,
  mode = "doc-avg",
  steps = 100,
  limit_range = NA_real_,
  compute_bootstrap_ci = FALSE,
  n_bt = 10L,
  seed = NULL,
  graded_relevance = FALSE,
  rename_graded_metrics = FALSE,
  propensity_scored = FALSE,
  label_distribution = NULL,
  cost_fp_constant = NULL,
  .ignore_relevance_warning = FALSE,
  .verbose = FALSE,
  .progress = FALSE
)
}
\arguments{
\item{gold_standard}{expects \code{data.frame} with cols
\emph{"label_id", "doc_id", "score"}}

\item{predicted}{multi-label prediction results. expects \code{data.frame}
with cols \emph{"label_id", "doc_id", "score"}}

\item{doc_strata}{a column that exists in either gold_standard or predicted,
that results should be grouped by, e.g. strata of document-space.
\code{doc_strata} is recommended to be of type factor, so that levels are
not implicitly dropped during bootstrap replications}

\item{label_dict}{two-column \code{data.frame} with col \emph{"label_id"}
and a second column that defines
groups of labels to stratify results by. Results in each stratum
will restrict gold_standard and predictions to the specified label-groups,
as if the vocabulary was consisting of the label group only.
All modes \code{c("doc-avg", "subj-avg", "micro") } are supported within
label-strata. Nevertheless, mixing
\code{mode = "doc-avg"} with fine-grained
label_strata can result in many missing values on document-level results.
Also rank-based thresholding (e.g. Top-5) will result in inhomogeneous
number of labels per documents within the defined label-strata.
\code{mode = "subj-avg"} or \code{mode = "micro"} can be more appropriate
in these circumstances.}

\item{mode}{aggregation mode: \emph{"doc-avg", "subj-avg", "micro"}}

\item{steps}{number of threshold-steps to use in auc-computation}

\item{limit_range}{a vector of limit values to apply on rank-column.
Defaults to NA, applying no cutoff on label-rank of predictions.}

\item{compute_bootstrap_ci}{logical indicator for computing bootstrap CIs}

\item{n_bt}{an integer number of resamples to undergo in bootstrapping}

\item{seed}{pass seed to make bootstrap replication reproducible}

\item{graded_relevance}{logical indicator for graded relevance. Defaults to
\code{FALSE} for binary relevance. If set to \code{TRUE}, the
\code{predicted} data.frame should contain a numeric column
\emph{"relevance"} with values in the range of \code{c(0, 1)}.}

\item{rename_graded_metrics}{if set to \code{TRUE}, the metric names in
the output will be prefixed with \emph{"g_"} to indicate that metrics
are computed with graded relevance.}

\item{propensity_scored}{logical, whether to use propensity scores as weights}

\item{label_distribution}{expects \code{data.frame} with cols
\emph{"label_id", "label_freq", "n_docs"}. \code{label_freq} corresonds to
the number of occurences a label has in the gold_standard. \code{n_docs}
corresponds to the total number of documents in the gold_standard.}

\item{cost_fp_constant}{Constant cost assigned to false positives.
cost_fp_constant must be
a numeric value > 0 or one of 'max', 'min', 'mean' (computed with reference
to the \code{gold_standard} label distribution). The default is NULL, i.e.
label weights are appplied to false positices as to false negatives and
true positives.}

\item{.ignore_relevance_warning}{logical, if graded_relevance = FALSE, but
column relevance is present in predicted, a warning can be silenced by
setting .ignore_relevance_warning = TRUE}

\item{.verbose}{logical indicator for verbose output, defaults to FALSE}

\item{.progress}{logical activating .progress bar in internal
\pkg{furrr}-computation}
}
\value{
a \code{data.frame} with cols pr_auc and (if applicable)
ci_lower, ci_upper and additional stratification variables
}
\description{
Compute Area under precision recall curve with support for bootstrap based
confidence intervals and different stratification and aggregation modes
for the underlying precision and recall aggregation
Precision is calculated as the best value at a given level of recall for
all possible thresholds on score and limits on rank. In essence,
compute_pr_auc performs a two dimensional optimisation over thresholds and
limits applying both threshold-based cutoff as well as rank-based cutoff.
}
\examples{
#' library(ggplot2)
library(casimir)

gold <- tibble::tribble(
~doc_id, ~label_id,
"A", "a",
"A", "b",
"A", "c",
"B", "a",
"B", "d",
"C", "a",
"C", "b",
"C", "d",
"C", "f"
)

pred <- tibble::tribble(
  ~doc_id, ~label_id, ~score, ~rank,
  "A", "a", 0.9, 1,
  "A", "d", 0.7, 2,
  "A", "f", 0.3, 3,
  "A", "c", 0.1, 4,
  "B", "a", 0.8, 1,
  "B", "e", 0.6, 2,
  "B", "d", 0.1, 3,
  "C", "f", 0.1, 3,
  "C", "c", 0.2, 1,
  "C", "e", 0.2, 1
)

auc <- compute_pr_auc(gold, pred, mode = "doc-avg")
}
\seealso{
compute_set_retrieval_scores compute_pr_auc_from_curve
}
